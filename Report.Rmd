---
title: "Report"
author: "Drunken Master 2"
date: "Due 5 November 2018"
output:
  pdf_document: default
  word_document: default
  html_document:
    df_print: paged
toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction 

Bootstrapping is an area of statistics that is usually implemented using simple Monte Carlo simulations whereby a certain calculation is repeated a large number of times with random sampling. Repeating calculations a large number of times, say 1,000,000 times, can become slow to compute. Therefore, the use of efficient and fast code is essential.

This is project aimed to improve and produce two fast and efficient bootstrap functions using R 3.5.1 (R, 2018) and SAS 9.4 (SAS Institute, Cary NC).

```{r load}
fitData <- read.csv("data/fitness.csv")
```

A short example analysis was given for each function. The fitness dataset from Rawlings (1998) contains measurements of the following seven variables obtained
from 31 men:
• Age: Age in years
• Weight: Weight in kg
• Oxygen: Oxygen intake rate, ml per kg body weight per minute
• RunTime: time to run 1.5 miles in minutes
• RestPulse: heart rate while resting
• RunPulse: heart rate at end of run
• MaxPulse: maximum heart rate recorded while running
All alnalyses in the following report were carried out using R 3.5.1 software.

\pagebreak

## R 

### *The function lmBoot*

The function *lmBoot* (Appendix A.###) uses bootstrap sampling methods to calculate estimates for the means and confidence intervals of the slope and intercept parameters produced by a linear regression. 

The function takes in two arguments:

- inputData: the dataset that will be used for sampling, where the response variable is in the first column and the remainder of the columns contain the covariates of interest.
- nBoot: The number of bootstrap samples to compute.

The function outputs:

- BootResults: An array with the number of rows equivalent to the nBoot argument and as many columns as there are Beta coeficients; i.e. for the intercept and covariates.
- ConfidenceIntervals: A matrix containing 95% confidence intervals for each parameter estimate.
 
### *Changes made to lmBoot*
1. The original lmBoot function only produced bootstrap samples for one covariate. This was changed so that lmBoot_2 produces bootstrap estimates for a multiple number of covariates and calculates confidence intervals for each parameter estimate. 

2. The use of the *lm* function was removed and the beta coefficients were calculated using matrix calculations instead. 

$$
\beta = (X^TX)^{-1}X^TY
$$

3. *forloops* are relatively slow and inefficient. Therefore, the *forloop* was replaced using *sapply* which applies a function to each element of a matrix. The function called *bootLM* was written to carry out the bootstrap algorithm. 

4. Parallelisation

Table ### illustrates differences in runtime for three different versions of the lmBoot function; the original, an improved version and a parallelised version. Each function was timed on how long it took to resample 100, 1000, 10000 and 100000 samples.

```{r rSpeed, echo = FALSE}
sasTiming <- matrix(c("100", 0.079, 0.006, 0.020,  
                      "1000", 0.941, 0.052, 0.026, 
                      "10000", 8.502, 0.564, 0.052, 
                      "100000", "-", 5.538, 0.370), byrow = TRUE, ncol = 4)
knitr::kable(sasTiming, digits = 3, caption = "Changes in Runtime (in seconds) of lmBoot", 
             col.names = c("Samples", "lmBoot", "lmBoot Improved", "lmBoot Parallised"))

```
Original boot fucntion (lmBoot) run speed for 100,000 samples = 127.289 secs
Final boot function (lmBoot_4) run speed for 100,000 samples = 0.37 secs
Overall speed increase was seen to be 126.919 secs. 

### *Microbenchmark*
From the table below, we could find that when the number of bootstrapping is low, the boot function performs better than lmBoot_par function. With increasing number of bootstrapping, lmBoot_par function performs much better than boot function. We could conclude that vectorization will imrpove function performance especially for compute-intensive problems.

```{r rmicro, echo = FALSE}
bootTiming <- matrix(c("10","998.0118", "24.1424",  
                      "100", "1003.773", "134.7217", 
                      "1000", "1029.438", "1273.774",
                      "10000", "1315.602", "11750.07"), byrow = TRUE, ncol = 3)
knitr::kable(bootTiming, digits = 4, caption = "Microbenchmark comparison (in milliseconds) between lmBoot_par and Boot", 
             col.names = c("Samples", "lmBoot_par", "Boot"))

```

### *Example analysis using lmBoot*



```{r rcode, echo = FALSE, fig.width = 10, fig.height = 12, fig.cap="\\label{fig:rcode} Bootstrap Distributions of Parameter Estimates" }

library(doParallel)
bootLM <- function(inputData, index){
  bootData <- inputData[sample(1:nrow(inputData), nrow(inputData), replace = T),]
  Xmat <- bootData[, -1]
  Ymat <- bootData[, 1]
  beta <- solve(t(Xmat)%*%Xmat)%*%t(Xmat)%*%Ymat
  return(t(beta))
}
lmBoot_4 <- function(inputData, nBoot){
  X <- cbind(1, inputData[, -1]) 
  sampleData <- as.matrix(cbind(inputData[, 1], X))
  
  nCores <- detectCores()
  myClust <- makeCluster(nCores - 1, type = "PSOCK")
  registerDoParallel(myClust)
  
  bootResults <- array(dim = c(nBoot, ncol(X)))
  bootResults <- parSapply(myClust, 1:nBoot, bootLM, inputData = sampleData)
  
  #bootResults <- plyr::ldply(bootResults)
  stopCluster(myClust)
  bootResults <- t(bootResults)
  
  return(bootResults)
}

library(dplyr)
par(mfrow = c(4,2))
testData <- fitData %>% select(Oxygen, everything())
set.seed(5763)
testResults <- lmBoot_4(testData, 10000)

#Plot the distributions for each parameter
  for(i in 1:ncol(testResults)){
    hist(testResults[, i], breaks = 50, 
         main = "", xlab = paste("Parameter Esimates of ", names(testResults[i])))
  }
  
  #Confidence intervals
  ciMatrix <- matrix(NA, nrow = ncol(testResults), ncol = 2)
  for(i in 1:ncol(testResults)){
    ciMatrix[i, ] <- quantile(testResults[,i], probs = c(0.025, 0.975))
  }
  colnames(ciMatrix) <- c("2.5%", "97.5%")
  rownames(ciMatrix) <- c("Intercept", names(testData[-1]))

knitr::kable(ciMatrix, digits = 3, caption = "95% Confidence Intervals", 
             col.names = c("2.5%", "97.5%"))
```
5. Interpretation of the Confidence Intervals of parameters estimates are as follows,
The Confidence Intervals of Age and Run time seem to have no skewness and may possibly even have normality.CI's of weights, Run time and Run pulse are right-skewed whereas the CI's of Rest pulse and Max pulse are left-skewed. All the CI's have observed to be distributed unimodally.
###tell me more stuff you want me to interpret


\pagebreak 

## SAS 

### *The program SASBoot*

The macro program *SASBoot* (Appendix A.###) uses bootstrap sampling methods to calculate estimates for the means and confidence intervals of the slope and intercept parameters produced by a linear regression. 

It takes in four agruments:

- NumberOfLoops: the number of bootstrap iterations.
- DataSet: A SAS dataset containing the response and covariate.
- XVariable: The covariate for our regression model (gen. continuous numeric).
- YVariable: The response variable for our regression model (gen. continuous numeric).

The program then outputs:

- ResultHolder: A SAS dataset with the number of rows equivalent to the NumberOfLoops argument and two columns; RandomIntercept and RandomSlope. 
- output.rtf: An RTF file containing 95% confidence intervals for the mean, the mean estimate for each parameter and plots of the distributions of the bootstrap parameters.
 
The function makes use of:

- MACRO statements to create a flexible program with input arguments.
- PROC SURVEYSELECT which allows the use of random sampling to generate random samples from a selected or inputed dataset.
- PROC REG to perform a linear regression.
 
### *Changes made to SASBoot*

The changes made to SASBoot were motivated, in part, by the work of Cassel (2018) in his paper "Don't Be Loopy: Re-Sampling and Simulation the SAS® Way".

1. The %do% loop was first removed and the following code was added to PROC SURVEYSELECT:  
*samprate = 1*  
*outhits*  
*rep = &NumberOfLoops*    

which ensures that NumberOfLoops samples of the same size as the original data set are produced and recorded.

2. A linear regression using PROC REG was improved by introducing the by-variable REPLICATE. This variable is automatically produced from PROC SURVEYSELECT to keep track of each new bootstrap sample, and ensures that the linear regression is run on each sample. Thus, only the Result Holder Dataset was necessary, and there was no need to generate the Temp Dataset.

3. The SASFILE statement was included to upload the dataset to RAM rather than the hard drive before any sampling was carried out so that the dataset does not have to be read-in every time a resample is done.

(4. Replaced noprint and ODS listing close - still working on this)


The program was run six times and Table ### displays the runtime (in seconds) for 1000 loops. The code used to measure the run time of the SASBoot program can be found in Appendix A.### (H, 2012).

```{r sasSpeed, echo = FALSE}
sasTiming <- matrix(c(183.3600, 0.2970, 6.0680, 
                      189.9730, 6.0870, 6.0870,
                      195.4200, 0.4220, 6.0640,
                      192.8350, 0.3130, 6.0410,
                      192.7860, 0.2960, 5.9410,
                      193.5580, 0.3140, 6.0620), byrow = TRUE, ncol = 3)
knitr::kable(sasTiming, digits = 3, caption = "Changes in Runtime of SASBoot", 
             col.names = c("RegBoot", " SASBoot ", "SASBoot (with rtf output)"))

```

### *Example analysis using SASBoot*

```{r SAS}
#Include plots and interpretation
```

An example analysis was conducted using the fitness datset. A linear model was set up with Oygen as the response and Weight as the covariate. The bootstrap 95% confidence intervals produced by SASBoot were then used to test the null hypothesis that there is no relationship between Oxygen and Weight, i.e. $\beta_i = 0$. 

If the confidence interval contains 0, one then fails to reject the null hypothesis and if it does not contain 0 one can reject the null hypothesis.

The confidence interval for the intercept term (36.4824, 73.1590) does not contain 0 which suggests that the estimator is not signicant at the 5% level. The confidence interval for the intercept term (-0.32842, 0.13533) does contain 0 which suggests that the estimator is signicant at the 5% level. 

\pagebreak 

## References
Cassell, D. (2018). Don't Be Loopy: Re-Sampling and Simulation the SAS® Way. [online]. Available at: http://www2.sas.com/proceedings/forum2007/183-2007.pdf [Accessed 26 Oct. 2018].

Donovan, C. (2018). MT5763 Project 2 - code collaboration and computer intensive inference. [Online].

H, J. (2012). To calculate SAS program run time. [online]. Available at: http://sashowto.blogspot.com/2012/06/to-calculate-sas-program-run-time.html [Accessed 26 Oct. 2018].

R Core Team (2018). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. Available at: https://www.R-project.org/.

SAS 9.4, SAS Institute Inc., Cary, NC, USA.

SAS Institute Inc. 2004. Proceedings of the Twenty-Ninth Annual SAS® Users Group International Conference. Cary, NC: SAS Institute Inc.

\pagebreak 

## Appendix

### A.### The lmBoot Function

```{r, eval = FALSE}
#The final code used for lmBoot
```

### A.### Code to measure program runtime in SAS

```{r, eval = FALSE}
%let _sdtm=%sysfunc(datetime());

  Program of interest to be timed

%let _edtm=%sysfunc(datetime());  
%let _runtm=%sysfunc(putn(&_edtm - &_sdtm, 12.4));  
%put It took &_runtm seconds to run the program;
```

### A.### The SASBoot Program

```{r, eval = FALSE}
#The final code used for SASBoot
```
